{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Preparation\n",
    "\n",
    "This notebook contains notes on how we should set up the ML pipeline for this project. It should perform the following steps:\n",
    "\n",
    "- [Load the data](#Load-the-model)\n",
    "- [Tokenize text data](#Tokenize-text-data)\n",
    "- [Build ML pipeline](#Build-ML-pipeline)\n",
    "- [Train ML pipeline](#Train-ML-pipeline)\n",
    "- [Test ML model](#Test-ML-model)\n",
    "- [Perform grid search](#Perform-grid-search)\n",
    "- [Test tuned ML model](#Test-tuned-ML-model)\n",
    "- [Further improvements](#Further-improvements)\n",
    "- [Export model](#Export-model)\n",
    "\n",
    "Note: This notebook must be run with python3 kernel since nltk is only compatible with Python3.\n",
    "\n",
    "##  Load the model\n",
    "\n",
    "We start by loading the model from the sqlite database we created in the etl pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql('SELECT * FROM DisasterResponse', engine)\n",
    "X = df.iloc[:, 0:4]\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text data\n",
    "\n",
    "For each message we perform the following:\n",
    "- Divide (tokenize) the text into individual words\n",
    "- Lemmatize each word back to its lemma (root)\n",
    "- Set each word to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to setup SSL to download ntlk packages\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download nltk packages\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes, lemmatizes and sets all characters to lowercase\n",
    "    for the input text.\n",
    "    \n",
    "    Input:\n",
    "    - text = string\n",
    "    \n",
    "    Output:\n",
    "    - clean_tokens = list of words that have been lemmatized and lowercase\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_token = lemmatizer.lemmatize(token).lower().strip()\n",
    "        clean_tokens.append(clean_token)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ML pipeline\n",
    "\n",
    "In this section we define the ML pipeline which should take in the message column and output classification results on the (36) categories column. The ML pipeline will perform the following:\n",
    "\n",
    "1. Tokenize each message based on the tokenize function we defined in the previous section, and use CountVectorizer to get the count of each word.\n",
    "2. Use TdidfTransfomer to compute tf-idf values based on the CountVectorizer results. \n",
    "3. Use MultiOutputClassifier with RandomForestClassifer to fit our model over multiple target variables (36 categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML pipeline\n",
    "\n",
    "In this step we split the dataset into training and testing data then fit the pipeline to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "pipeline.fit(X_train['message'], y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ML model\n",
    "\n",
    "In this step we run the model on our test data then for each category, we calculate the following:\n",
    "- Recall = TP / (TP + FN) = out of all actual positive classes, how many did we predict correctly\n",
    "- Precision = TP / (TP + FP) = out of all positive classes we predicted, how many are actually positive\n",
    "- Accuracy = (TP + TN) / Total = out of all cases how many did we predict correctly\n",
    "- F1 score  = 2 * Recall * Precision / (Recall + Precision) = measures recall and precision at the same time by using their harmonic mean\n",
    "\n",
    "Note that we use the weighted average of the two classes (0s or 1s) within each category. This accounts for the fact that one class may be dominant over another within a given category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the predictions\n",
    "y_pred = pipeline.predict(X_test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate and plot recall, precision, f1-score and accuracy\n",
    "    \n",
    "    Input:\n",
    "    - y_test = test response vector\n",
    "    - y_pred = predicted response vector\n",
    "    \n",
    "    Output:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Go through each category, calculate and store the QOIs\n",
    "    recall    = []\n",
    "    precision = []\n",
    "    f1_score  = []\n",
    "    accuracy  = []\n",
    "    for i in range(0, 36):\n",
    "        result = classification_report(y_test.iloc[:, i], [x[i] for x in y_pred], output_dict=True)\n",
    "        recall.append(result['weighted avg']['recall'])\n",
    "        precision.append(result['weighted avg']['precision'])\n",
    "        f1_score.append(result['weighted avg']['f1-score'])\n",
    "        accuracy.append(result['accuracy'])\n",
    "    \n",
    "    # Sort QOIs for plotting\n",
    "    recall.sort()\n",
    "    precision.sort()\n",
    "    f1_score.sort()\n",
    "    accuracy.sort()\n",
    "    plt.plot(recall, label='recall')\n",
    "    plt.plot(precision, label='precision')\n",
    "    plt.plot(f1_score, label='f1-score')\n",
    "    plt.plot(accuracy, label='accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    print('Min recall: {}, Max recall: {}'.format(recall[0], recall[-1]))\n",
    "    print('Min precision: {}, Max precision: {}'.format(precision[0], precision[-1]))\n",
    "    print('Min f1_score: {}, Max f1_score: {}'.format(f1_score[0], f1_score[-1]))\n",
    "    print('Min accuracy: {}, Max accuracy: {}'.format(accuracy[0], accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot and display the min and max of each QOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform grid search\n",
    "\n",
    "Use grid search to tune the hyperparameters in the previous pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define list of parameters and values to test in grid search\n",
    "# parameters = {\n",
    "#     'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#     'vect__max_features': (None, 5000, 10000),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'clf__estimator__n_estimators': [50, 100, 200],\n",
    "#     'clf__estimator__min_samples_split': [2, 3, 4]\n",
    "# }\n",
    "\n",
    "# Reduce parameter tuning to save time\n",
    "parameters = {\n",
    "    'vect__max_features': (5000, 10000),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__estimator__n_estimators': [100, 200],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "cv.fit(X_train['message'], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tuned ML model\n",
    "\n",
    "Calculate the same QOI as before but for the tuned ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using the tuned model\n",
    "y_pred_cv = cv.predict(X_test['message'])\n",
    "\n",
    "display_results(y_test, y_pred_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements\n",
    "\n",
    "In this section we try to further improve the model using an alternative classifier algorithm known as XGBoost. Future work should perform hyperparameter tuning for XGBoost as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(XGBClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model\n",
    "\n",
    "In this section, we export our best-performing model as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(cv, open('classifier.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
